# 背景

指标是事件响应的关键, 警报通常写为时间序列的条件。 但是由于需要预先声明并限制基数，指标通常只能用于揭示预期的行为。 因此，指标只能说明一半的问题； 为了全面了解事件的原因，工程师通常求助于日志（程序的文本输出）来获取更详细的信息。

通常的事件响应首先从警报开始，然后查看仪表板（可能还需要一些特定的查询），最后再查明错误来自的确切服务、主机或实例。 然后，工程师将尝试查找该服务/主机/实例和时间范围的日志以找到根本原因。 当前的现状是将指标和日志存储在两个不同的系统中，因此工程师需要将查询从一种语言和界面换成另外一种。

因此，Loki设计的第一个假设是，最大限度地减少日志和指标之间的上下文切换成本将有助于减少事件响应时间并改善用户体验。

# 现有的办法

日志聚合并不是一个新想法，像时间序列监控一样，有很多 SaaS 厂商和开源项目在争夺注意力。 几乎所有现有的解决方案都涉及使用全文搜索系统来索引日志；乍一看，这似乎是显而易见的解决方案，具有丰富而强大的功能集，允许进行复杂的查询。

这些现有的解决方案难以扩展、资源密集且操作复杂。 如上所述，一种越来越普遍的模式是结合使用时间序列监控和日志聚合——但是由于查询工具灵活性和复杂性通常没有被普遍采用； 大多数查询只关注时间范围和一些简单的参数（主机、服务等）。 使用这些系统进行日志聚合有点杀鸡用牛刀的感觉。

现有系统的挑战和运营开销已导致许多买家落入 SaaS 运营商手中。 因此，该设计的第二个假设是，在易操作性和查询语言的复杂性之间更偏向易操作性。

# 成本效益

随着向 SaaS 日志聚合的迁移，此类系统的过高成本变得更加明显。 这种成本不仅来自用于实现全文搜索的技术 - 对倒排索引进行缩放和分片很困难； 要么写入触及每个分片，要么读取必须 - 而且还来自操作的复杂性。

寻求日志聚合系统的买家的常见经验是，在收到超出他们愿意花费的数量级的现有日志负载的初始报价后，转向工程师并要求他们减少日志。 由于存在日志记录以涵盖意外错误（见上文），工程师的典型反应是难以置信 - “如果我必须意识到我记录的内容，那么记录日志的意义何在？”。

最近出现的一些系统在这里提供了不同的权衡设计。 Peter Bourgon 的开源 [OKLOG](https://github.com/oklog/oklog) 项目（已归档）避开基于时间的所有形式的索引，并采用最终一致的基于网格的分发策略。 这两个设计决策提供了巨大的成本降低和从根本上更简单的操作，但我们认为不符合我们的其他设计要求 - 查询不够便利且过于复杂。 然而，我们确实认识到这是一个有吸引力的本地部署方案。

因此，第三个假设是，一个显着更具成本效益的解决方案，具有稍微不同的索引权衡，将是一个非常大的问题......


# Kubernetes

考虑现代云原生/微服务/容器化工作负载中日志记录的变化是一个有趣的问题。 现在的标准模式是应用程序只需将日志写入 STDOUT 或 STDERR。 Kubernetes 和 Docker 等平台以此为基础提供有限的日志聚合功能； 日志本地存储在节点上，可以使用标签选择器按需获取和聚合。

但是对于这些简单的系统，当 pod 或节点消失时，日志通常会丢失。 这通常是用户意识到他们需要日志聚合的第一个触发因素 - 一个 pod 或节点神秘地死亡，并且没有可用的日志来诊断原因。

# Prometheus and Cortex

最后值得一提的是普罗米修斯是如何融入这个事件的。 Prometheus 是一个以时间序列数据库为中心的监控系统。 TSDB 使用一组键值对索引样本集合（时间序列）。 可以通过指定这些标签的子集（匹配器）来查询这些时间序列，返回与这些标签匹配的所有时间序列。 通过使查询对新标签或更改标签的存在具有鲁棒性，这与传统 Graphite 分层标签之类的区别。

在 Prometheus（和 Cortex）中，这些标签存储在倒排索引中，可以快速查询这些标签。 Cortex 中的这种倒排索引存在于内存中以存储最近的数据，并存在于分布式 KV 存储（BigTable、DynamoDB 或 Cassandra）中以存储历史数据。 Cortex 索引通过保留和吞吐量线性扩展，但设计受限于任何给定标签的基数。

Prometheus 系统包含许多组件，但本次讨论的一个值得注意的组件是 [mtail](https://github.com/google/mtail)。 Mtail 允许您“从应用程序日志中提取白盒监控数据以收集到时间序列数据库中”。 这允许您为不公开任何本机指标的应用程序构建时间序列监控和警报。

# 用户场景

- 在收到有关我的服务的警报并深入了解与该警报相关的查询后，我想快速查看与在警报时产生这些时间序列的作业相关的日志。

- 在 pod 或节点消失后，我希望能够从消失之前检索日志，以便诊断它消失的原因。

- 在发现我的服务存在持续问题后，我想从一些日志中提取一个指标并将其与我现有的时间序列数据相结合。

- 有一个老旧项目，它不公开有关错误的指标 - 它只记录它们。 我想根据日志中错误的发生率构建警报。

# 非目标

日志聚合系统的一个常见用例是存储结构化的、基于事件的数据——例如，为系统的每个请求发出一个事件，并包括所有请求详细信息和元数据。 通过此类部署，您可以提出诸如“向我展示具有最高 99% 延迟的前 10 个用户”之类的问题，由于用户的高基数，您通常无法使用时间序列度量系统来做到这一点。 虽然这个用例是完全有效的，但它不是我们在这个系统中的目标。


# 解决方案

我们将构建一个托管日志聚合系统，它索引与这些日志流相关的元数据，而不是索引日志流本身的内容。 该元数据将采用 Prometheus 风格的多维标签的形式。 这些标签将与与从作业中提取的时间序列/指标相关联的标签一致，以便可以使用相同的标签从作业中查找日志，也可以用于从所述作业中查找时间序列，从而实现在查询界面上快速切换查询。

该系统不会解决许多通常与日志聚合相关的复杂分布式系统和存储挑战，而是会将它们卸载到现有的分布式数据库和对象存储系统中。 这将通过让大多数系统服务成为无状态和短暂的，并允许系统运营商使用云供应商提供的托管服务来降低运营复杂性。

通过仅索引与日志流相关的元数据，系统将索引负载减少多个数量级 - 预计可能 100MB 的日志数据有大约 1KB 的元数据。 实际的日志数据将存储在托管对象存储服务（S3、GCS 等）中，由于供应商之间的竞争，这些服务面临着巨大的成本下行压力。 我们将能够转嫁这些节省，并以比竞争对手低几个数量级的价格提供该系统。 例如，GCS 的成本为 0.026 美元/GB/月，而 Loggly 的成本约为 100 美元/GB/月。

由于这是一个托管系统，因此在客户端主机或 pod 失败后，日志将很容易获得。 代理将部署到客户端系统中的每个节点，以将日志发送到我们的服务，并确保元数据与指标一致。


# 架构

# Agent
第一个挑战是获取与时间序列/指标相关的元数据一致的可靠元数据。 为了实现这一点，我们将使用与 Prometheus 相同的服务发现和标签重新标记库。 这将被打包在一个守护进程中，该守护进程发现目标、生成元数据标签并跟踪日志文件以生成日志流，这些日志将暂时在客户端缓冲，然后发送到服务。 鉴于对节点故障的最新日志的要求，它可以执行的批处理量有一个基本限制。

该组件已存在并命名为 Promtail。


# 日志写入

写入路径上的服务器端组件与 Cortex 架构类似：

- 写入将首先命中分发器（Distributor），分发器负责将写入分发和复制到摄取者。 我们将使用 Cortex 一致的哈希环； 我们将基于整个元数据（包括用户 ID）的散列分配写入，因为日志流没有方便的度量名称。

- 接下来的写入将命中一个摄取器（Ingester），它将内存中相同流的写入批处理成“日志块”。 当块达到预定义的大小或年龄时，它们会定期刷新到 Cortex 块存储。

- Cortex 块存储将被更新以减少在读写路径上块数据的复制，并添加对写入 GCS 块的支持。


# 日志块

分片(chunk)格式对系统的成本和性能很重要。 块是给定标签集在特定时期内的所有日志。 分片必须支持追加、查找和流式读取。 假设一个“平均”节点每天将产生 10 GB 的日志，并且平均运行 30 个容器，那么每个日志流将以 4 KB/s 的速度写入。 日志数据的预期压缩率应该是 10 倍左右。

在选择最佳块大小时，我们需要考虑：

- 操作成本 vs 存储成本； 在小对象大小时，每次操作的成本占主导地位，并且将它们存储在数据库（例如 Bigtable）中更便宜。
- 分片索引加载 - 每个分片都需要索引中的条目； 运行 Cortex 的经验告诉我们，这是运行系统的最大成本组件，但考虑到更大的分片大小，我怀疑这里不会出现这种情况。
- 构建块的内存成本和丢失风险，这可能是限制因素。 我们应该期望能够为每台机器处理 1000 台主机的流，以便能够经济高效地运行服务； 如果每个流需要 1MB 的内存并且每个主机需要 30 个流，这意味着每个 Ingester 需要 30GB 的内存（WAL 类似）。 1000 台主机也意味着 130MB/s 的入站和出站带宽以及入站压缩。
- 压缩效率——在非常小的尺寸下，压缩是无效的； 日志行将需要一起批处理以实现更接近最佳压缩。

例如，12 小时的日志数据将产生约 100 MB 未压缩和约 10 MB 压缩的分片。 12 小时也是我们在 Cortex 中使用的分片长度的上限。 考虑到构建这些的内存要求，接近 1 MB（压缩）的分片大小看起来更有可能。

该提议是由一系列块(block)组成的分片(chunk)； 第一个块是gorilla-style的编码时间索引，随后的块包含压缩的日志数据。 一旦产生了足够多的块以形成足够大的块，它们将被附加在一起以形成一个分片。 需要进行一些实验才能找到正确的分片格式。


# 日志查询

由于分片比 Prometheus/Cortex 分片大许多数量级（Cortex 分片的大小最大为 1KB），因此无法加载它们并对其进行整体解压缩。 因此，我们需要支持流式传输和迭代它们，只解压缩我们需要的部分。 同样，这里有很多细节需要解决，但我怀疑积极使用 gRPC 流和堆将是有序的（请参阅新的 Cortex 迭代查询 PR）